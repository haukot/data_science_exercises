{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vpython import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# Name:        AcrobotDemo.py\n",
    "# Description: A module of Reinforment Learning tools\n",
    "#\n",
    "# Author:      Jose Antonio Martin H. (JAMH)\n",
    "# Contact:     <jamartin@dia.fi.upm.es>\n",
    "#\n",
    "# Created:     22/05/2010\n",
    "# Copyright:   (c) Jose Antonio Martin H. 2010\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "#-------------------------------------------------------------------------------\n",
    "#!/usr/bin/env python\n",
    "\n",
    "#from rltools.FARLBasic import *\n",
    "#from Environments.AcrobotEnvironmentG import AcrobotEnvironment\n",
    "from rltools.kNNSCIPY import kNNQ\n",
    "from rltools.ActionSelection import *\n",
    "import cPickle\n",
    "\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from ivisual import *\n",
    "# from ivisual.graph import *\n",
    "\n",
    "class FARLBase:\n",
    "\n",
    "    def __init__(self,Q,Environment,Action_Selector,gamma=1.0):\n",
    "\n",
    "        self.gamma               = gamma    #discount factor\n",
    "        self.Environment         = Environment\n",
    "        self.nactions            = Environment.nactions  # number of actions\n",
    "        self.Q                   = Q #the function approximator\n",
    "        self.SelectAction        = Action_Selector # the action_selection function\n",
    "        self.SelectAction.parent = self\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Default graphic\n",
    "        x_width  = 400\n",
    "        y_height = 300\n",
    "        self.LearningCurveGraph = graph(x=0, y=y_height, width=2*x_width, height=y_height,\n",
    "                        title=\"Learning Curve\", xtitle=\"Episode\", ytitle=\"Steps\",\n",
    "                        xmin=0.0, ymin=0.0, foreground=color.black, background=color.white)\n",
    "        #Learning Curve\n",
    "        self.Lcurve = gcurve(graph = self.LearningCurveGraph,color=color.blue)\n",
    "        #self.gtitle = label (canvas = self.LearningCurveGraph.display, pos=self.LearningCurveGraph.display.center,opacity=0, text=\"Learning Curve\")\n",
    "        #self.LearningCurveGraph.display.visible= False\n",
    "\n",
    "\n",
    "    def SARSAEpisode(self, maxsteps=100):\n",
    "        # do one episode with sarsa learning\n",
    "        # maxstepts: the maximum number of steps per episode\n",
    "        # Q: the current QTable\n",
    "        # alpha: the current learning rate\n",
    "        # gamma: the current discount factor\n",
    "        # epsilon: probablity of a random action\n",
    "        # statelist: the list of states\n",
    "        # actionlist: the list of actions\n",
    "\n",
    "        s                = self.Environment.GetInitialState()\n",
    "        steps            = 0\n",
    "        total_reward     = 0\n",
    "        r                = 0\n",
    "        # selects an action using the epsilon greedy selection strategy\n",
    "        a,v   = self.SelectAction(s)\n",
    "\n",
    "        for i in range(1,maxsteps+1):\n",
    "\n",
    "\n",
    "            # do the selected action and get the next car state\n",
    "            sp     = self.Environment.DoAction(a,s)\n",
    "\n",
    "\n",
    "            # observe the reward at state xp and the final state flag\n",
    "            r,isfinal    = self.Environment.GetReward(sp)\n",
    "            total_reward = total_reward + r\n",
    "\n",
    "            # select action prime\n",
    "            ap,vp     = self.SelectAction(sp)\n",
    "\n",
    "\n",
    "            # Update the Qtable, that is,  learn from the experience\n",
    "            #target_value = r + self.gamma*self.Q(sp,ap) * (not isfinal)\n",
    "            target_value = r + self.gamma* vp * (not isfinal)\n",
    "            #self.Q.Addtrace(s,a,r)\n",
    "            self.Q.Update(s,a,target_value)\n",
    "\n",
    "\n",
    "\n",
    "            #update the current variables\n",
    "            s = sp\n",
    "            a = ap\n",
    "\n",
    "            #increment the step counter.\n",
    "            steps = steps + 1\n",
    "\n",
    "            # if reachs the goal breaks the episode\n",
    "            if isfinal==True:\n",
    "                break\n",
    "\n",
    "        return total_reward,steps\n",
    "\n",
    "    def kNNCQEpisode(self, maxsteps=100):\n",
    "        # do one episode with sarsa learning\n",
    "        # maxstepts: the maximum number of steps per episode\n",
    "        # Q: the current QTable\n",
    "        # alpha: the current learning rate\n",
    "        # gamma: the current discount factor\n",
    "        # epsilon: probablity of a random action\n",
    "        # statelist: the list of states\n",
    "        # actionlist: the list of actions\n",
    "\n",
    "        s                = self.Environment.GetInitialState()\n",
    "        steps            = 0\n",
    "        total_reward     = 0\n",
    "        r                = 0\n",
    "        # selects an action using the epsilon greedy selection strategy\n",
    "        action,a   = self.Q.GetActionList(s)\n",
    "\n",
    "        for i in range(1,maxsteps+1):\n",
    "\n",
    "\n",
    "            # do the selected action and get the next car state\n",
    "            sp     = self.Environment.DoAction(action,s)\n",
    "\n",
    "\n",
    "            # observe the reward at state xp and the final state flag\n",
    "            r,isfinal    = self.Environment.GetReward(sp,action)\n",
    "            total_reward = total_reward + r\n",
    "\n",
    "            # select action prime\n",
    "            actionp,ap   = self.Q.GetActionList(sp)\n",
    "\n",
    "            # Update the Qtable, that is,  learn from the experience\n",
    "            target_value = r + self.gamma*self.Q(sp) * (not isfinal)\n",
    "            self.Q.Update(s,a,target_value)\n",
    "\n",
    "\n",
    "            #update the current variables\n",
    "            s      = sp\n",
    "            a      = ap\n",
    "            action = actionp\n",
    "\n",
    "            #increment the step counter.\n",
    "            steps = steps + 1\n",
    "\n",
    "            # if reachs the goal breaks the episode\n",
    "            if isfinal==True:\n",
    "                break\n",
    "\n",
    "        return total_reward,steps\n",
    "\n",
    "\n",
    "\n",
    "    def NeuroQEpisode(self, maxsteps=100):\n",
    "        # do one episode with sarsa learning\n",
    "        # maxstepts: the maximum number of steps per episode\n",
    "        # Q: the current QTable\n",
    "        # alpha: the current learning rate\n",
    "        # gamma: the current discount factor\n",
    "        # epsilon: probablity of a random action\n",
    "        # statelist: the list of states\n",
    "        # actionlist: the list of actions\n",
    "\n",
    "        s                = self.Environment.GetInitialState()\n",
    "        steps            = 0\n",
    "        total_reward     = 0\n",
    "        r                = 0\n",
    "        # selects an action using the epsilon greedy selection strategy\n",
    "        a   = self.Q.GetAction(s)\n",
    "\n",
    "        for i in range(1,maxsteps+1):\n",
    "\n",
    "\n",
    "            # do the selected action and get the next car state\n",
    "            sp     = self.Environment.DoAction(a,s)\n",
    "\n",
    "\n",
    "            # observe the reward at state xp and the final state flag\n",
    "            r,isfinal    = self.Environment.GetReward(sp)\n",
    "            total_reward = total_reward + r\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Update the Qtable, that is,  learn from the experience\n",
    "            self.Q.Update(s,a,r)\n",
    "            # select action prime\n",
    "            a     = self.Q.GetAction(sp)\n",
    "\n",
    "            #update the current variables\n",
    "            s = sp\n",
    "\n",
    "\n",
    "            #increment the step counter.\n",
    "            steps = steps + 1\n",
    "\n",
    "            # if reachs the goal breaks the episode\n",
    "            if isfinal==True:\n",
    "                break\n",
    "\n",
    "        return total_reward,steps\n",
    "\n",
    "    def QLearningEpisode(self, maxsteps=100 ):\n",
    "        \"\"\" do one episode of QLearning \"\"\"\n",
    "        # maxstepts: the maximum number of steps per episode\n",
    "        # alpha: the current learning rate\n",
    "        # gamma: the current discount factor\n",
    "        # epsilon: probablity of a random action\n",
    "        # actionlist: the list of actions\n",
    "\n",
    "\n",
    "        s                = self.Environment.GetInitialState()\n",
    "        steps            = 0\n",
    "        total_reward     = 0\n",
    "\n",
    "        for i in range(1,maxsteps+1):\n",
    "\n",
    "            # selects an action using the epsilon greedy selection strategy\n",
    "            a = self.SelectAction(s)\n",
    "\n",
    "\n",
    "            # do the selected action and get the next state\n",
    "            sp     = self.Environment.DoAction( a,s )\n",
    "\n",
    "            # observe the reward at state xp and the final state flag\n",
    "            r,isfinal    = self.Environment.GetReward(sp)\n",
    "            total_reward = total_reward + r\n",
    "\n",
    "            # Update the Qtable, that is,  learn from the experience\n",
    "            vp = self.Q(sp)\n",
    "            target_value = r + self.gamma * max(vp) * (not isfinal)\n",
    "            sp = self.Q.Update(s,a,target_value)\n",
    "\n",
    "            #update the current variables\n",
    "            s = sp\n",
    "\n",
    "            #increment the step counter.\n",
    "            steps = steps + 1\n",
    "\n",
    "            # if reachs the goal breaks the episode\n",
    "            if isfinal==True:\n",
    "                break\n",
    "\n",
    "        return total_reward,steps\n",
    "\n",
    "    def PlotLearningCurve(self,episode,steps,epsilon):\n",
    "\n",
    "        #self.LearningCurveGraph.canvas.visible= True\n",
    "        #self.gtitle.pos = self.LearningCurveGraph.canvas.center\n",
    "        self.Lcurve.plot(pos=(episode,steps))\n",
    "        self.gtitle.text =\" Steps: \"+str(steps)+\" epsilon: \"+str(round(epsilon,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "# from ivisual import *\n",
    "# from ivisual.graph import *\n",
    "from vpython import *\n",
    "\n",
    "\n",
    "class AcrobotEnvironment:\n",
    "    \"\"\"\n",
    "    This class implements Acrobot Environment\n",
    "    @author    Jose Antonio Martin H.\n",
    "    @version   1.0\n",
    "    \"\"\"\n",
    "\n",
    "    title =\"Acrobot Balancing\"\n",
    "\n",
    "\n",
    "    input_ranges  = [[-pi,pi],[-pi,pi],[-4*pi,4*pi],[-9*pi,9*pi]]\n",
    "    reward_ranges = [[-1.0,1000.0]]\n",
    "    deep_in = [10,10,5,5] #2^n partitions\n",
    "    deep_out= [10] #2^n different partitions\n",
    "\n",
    "\n",
    "\n",
    "    # The current real values of the state\n",
    "    maxSpeed1 = 4*pi\n",
    "    maxSpeed2 = 9*pi\n",
    "    m1        = 1.0\n",
    "    m2        = 1.0\n",
    "    l1        = 1.0\n",
    "    l2        = 1.0\n",
    "    l1Square  = l1*l1\n",
    "    l2Square  = l2*l2\n",
    "    lc1       = 0.5\n",
    "    lc2       = 0.5\n",
    "    lc1Square = lc1*lc1\n",
    "    lc2Square = lc2*lc2\n",
    "    I1        = 1.0\n",
    "    I2        = 1.0\n",
    "    g         = 9.8\n",
    "    delta_t   = 0.05\n",
    "\n",
    "\n",
    "\n",
    "    #The number of actions.\n",
    "    action_list = (-1.0 , 0.0 , 1.0)\n",
    "    nactions    = len(action_list)\n",
    "\n",
    "    #Flag which is set to true when goal was reached.\n",
    "    reset = False\n",
    "\n",
    "    # number of steps of the current trial\n",
    "    steps = 0\n",
    "\n",
    "    # number of the current episode\n",
    "    episode = 0\n",
    "\n",
    "    # do you want to show nice graphs?\n",
    "    graphs = True\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.InitGraphs()\n",
    "\n",
    "\n",
    "\n",
    "    def UpdateState(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def GetInitialState(self):\n",
    "        s = array([0, 0, 0, 0])\n",
    "        self.StartEpisode()\n",
    "\n",
    "        return  s\n",
    "\n",
    "    def StartEpisode(self):\n",
    "        self.steps   = 0\n",
    "        self.episode = self.episode + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def getState(self):\n",
    "         \"\"\"\n",
    "          Returns the current situation.\n",
    "          A situation can be the current perceptual inputs, a random problem instance ...\n",
    "         \"\"\"\n",
    "         pass\n",
    "\n",
    "\n",
    "    def GetReward(self, x ):\n",
    "        # r: the returned reward.\n",
    "        # f: true if the car reached the goal, otherwise f is false\n",
    "        y_acrobot = [0,0,0]\n",
    "\n",
    "        theta1 = x[0]\n",
    "        theta2 = x[1]\n",
    "        y_acrobot[1] = y_acrobot[0] - cos(theta1)\n",
    "        y_acrobot[2] = y_acrobot[1] - cos(theta2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #goal\n",
    "        goal = y_acrobot[0] + 1.5\n",
    "\n",
    "        #r = y_acrobot[2]\n",
    "        r = -1\n",
    "        f = False\n",
    "\n",
    "        if  y_acrobot[2] >= goal:\n",
    "        \t#r = 10*y_acrobot[2]\n",
    "            r = 0\n",
    "            f = True\n",
    "\n",
    "\n",
    "        return r,f\n",
    "\n",
    "\n",
    "    def DoAction(self, a, x ):\n",
    "\n",
    "        self.steps = self.steps+1\n",
    "        torque      = self.action_list[a]\n",
    "\n",
    "        # Parameters for simulation\n",
    "        theta1,theta2,theta1_dot,theta2_dot = x\n",
    "\n",
    "\n",
    "        for i in range(4):\n",
    "            d1     = self.m1*self.lc1Square + self.m2*(self.l1Square + self.lc2Square + 2*self.l1*self.lc2 * cos(theta2)) + self.I1 + self.I2\n",
    "            d2     = self.m2*(self.lc2Square+self.l1*self.lc2*cos(theta2)) + self.I2\n",
    "\n",
    "            phi2   = self.m2*self.lc2*self.g*cos(theta1+theta2-pi/2.0)\n",
    "            phi1   = -self.m2*self.l1*self.lc2*theta2_dot*sin(theta2)*(theta2_dot-2*theta1_dot)+(self.m1*self.lc1+self.m2*self.l1)*self.g*cos(theta1-(pi/2.0))+phi2\n",
    "\n",
    "            accel2 = (torque+phi1*(d2/d1)-self.m2*self.l1*self.lc2*theta1_dot*theta1_dot*sin(theta2)-phi2)\n",
    "            accel2 = accel2/(self.m2*self.lc2Square+self.I2-(d2*d2/d1))\n",
    "            accel1 = -(d2*accel2+phi1)/d1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \t    theta1_dot = theta1_dot + accel1*self.delta_t\n",
    "\n",
    "    \t    if theta1_dot<-self.maxSpeed1:\n",
    "                theta1_dot=-self.maxSpeed1\n",
    "\n",
    "    \t    if theta1_dot>self.maxSpeed1:\n",
    "                theta1_dot=self.maxSpeed1\n",
    "\n",
    "\n",
    "    \t    theta1     =  theta1 + theta1_dot*self.delta_t\n",
    "    \t    theta2_dot =  theta2_dot + accel2*self.delta_t\n",
    "\n",
    "\n",
    "    \t    if theta2_dot<-self.maxSpeed2:\n",
    "                theta2_dot=-self.maxSpeed2\n",
    "\n",
    "    \t    if theta2_dot>self.maxSpeed2:\n",
    "                theta2_dot=self.maxSpeed2\n",
    "\n",
    "    \t    theta2 = theta2 + theta2_dot*self.delta_t\n",
    "\n",
    "\n",
    "\n",
    "##\tif theta1<-pi:\n",
    "##\t    theta1 = -pi\n",
    "##\n",
    "##        if theta1>pi:\n",
    "##            theta1 = pi\n",
    "\n",
    "        if theta2<-pi:\n",
    "            theta2 = -pi\n",
    "\n",
    "        if theta2>pi:\n",
    "            theta2 = pi\n",
    "\n",
    "\n",
    "\n",
    "        xp = [theta1,theta2,theta1_dot,theta2_dot]\n",
    "\n",
    "        if self.graphs:\n",
    "            if not self.scene.visible:\n",
    "                self.scene.visible=True\n",
    "            self.PlotFunc(xp,torque,self.steps)\n",
    "\n",
    "        return xp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #------------------------------------ PLOT FUNCTIONS -----------------------------\n",
    "\n",
    "\n",
    "    def InitGraphs(self):\n",
    "\n",
    "        x_width  = 400\n",
    "        y_height = 300\n",
    "\n",
    "        self.scene = canvas(x=0, width=x_width, height=y_height,\n",
    "                             title=\"Acrobot\",visible=False)\n",
    "\n",
    "        self.scene.autoscale=0\n",
    "\n",
    "        theta1 = 0\n",
    "        theta2 = 0\n",
    "        x_acrobot = [0,0,0]\n",
    "        y_acrobot = [0,0,0]\n",
    "\n",
    "        x_acrobot[0]=0\n",
    "        y_acrobot[0]=0\n",
    "\n",
    "        x_acrobot[1] = x_acrobot[0] + sin(theta1)*2\n",
    "        y_acrobot[1] = y_acrobot[0] - cos(theta1)*2\n",
    "\n",
    "        x_acrobot[2] = x_acrobot[1] + sin(theta2)*2\n",
    "        y_acrobot[2] = y_acrobot[1] - cos(theta2)*2\n",
    "\n",
    "\n",
    "        self.r1    = sphere(diplsay = self.scene,pos=vec(0,0,0), radius=0.3)\n",
    "        self.r2    = sphere(diplsay = self.scene,pos=vec(x_acrobot[1],y_acrobot[1],0), radius=0.3)\n",
    "        self.r3    = sphere(diplsay = self.scene,pos=vec(x_acrobot[2],y_acrobot[2],0), radius=0.3)\n",
    "\n",
    "        self.link1 = curve(diplsay = self.scene,pos=[self.r1.pos,self.r2.pos], radius=0.1)\n",
    "        self.link2 = curve(diplsay = self.scene,pos=[self.r2.pos,self.r3.pos], radius=0.1)\n",
    "\n",
    "        self.top   = arrow(diplsay = self.scene,pos=vec(0,6,0), axis=vec(0,-2,0), shaftwidth=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def PlotFunc(self,s,a,steps):\n",
    "\n",
    "        theta1 = s[0]\n",
    "        theta2 = s[1]\n",
    "\n",
    "        x_acrobot = [0,0,0]\n",
    "        y_acrobot = [0,0,0]\n",
    "\n",
    "        x_acrobot[0]=0\n",
    "        y_acrobot[0]=0\n",
    "\n",
    "        x_acrobot[1] = x_acrobot[0] + sin(theta1)*2\n",
    "        y_acrobot[1] = y_acrobot[0] - cos(theta1)*2\n",
    "\n",
    "        x_acrobot[2] = x_acrobot[1] + sin(theta2)*2\n",
    "        y_acrobot[2] = y_acrobot[1] - cos(theta2)*2\n",
    "\n",
    "\n",
    "        self.r2.pos = vec(x_acrobot[1],y_acrobot[1],0)\n",
    "        self.r3.pos = vec(x_acrobot[2],y_acrobot[2],0)\n",
    "\n",
    "        self.link1.pos = [self.r1.pos,self.r2.pos]\n",
    "        self.link2.pos = [self.r2.pos,self.r3.pos]\n",
    "\n",
    "         #update canvas\n",
    "        #self.scene.title = \"Steps: \"+str(steps)+ \" theta1: \"+str(round(theta1,2)) + \" theta2: \"+str(round(theta2,2))+ \"-- Altitude: \"+str(round(y_acrobot[2],2))\n",
    "\n",
    "\n",
    "        rate(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "object does not have a \"pos\" attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-2ffeb64a4552>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/haukot/apps/anaconda/lib/python2.7/site-packages/vpython/vpython.pyc\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, val)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1708\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1709\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object does not have a \"pos\" attribute'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1711\u001b[0m     \u001b[1;31m# def __del__(self):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: object does not have a \"pos\" attribute"
     ]
    }
   ],
   "source": [
    "a.pos = vec(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def AcrobotExperiment(Episodes=100,nk=1):\n",
    "    print\n",
    "    print(\"           INIT EXPERIMENT\", \"k=\"+str(nk))\n",
    "\n",
    "    # results of the experiment\n",
    "    x = range(1,Episodes+1)\n",
    "    y =[]\n",
    "\n",
    "    #Build the Environment\n",
    "    ACEnv = AcrobotEnvironment()\n",
    "\n",
    "    # Build a function approximator\n",
    "    Q = kNNQ(nactions=ACEnv.nactions,input_ranges=ACEnv.input_ranges,nelemns=[11,11,11,11],npoints=False,k=2**4,alpha=5.0,lm=0.90)\n",
    "\n",
    "    #Q.Q+=10000\n",
    "    #Q = kNNQ(nactions=ACEnv.nactions,input_ranges=ACEnv.input_ranges,nelemns=False,npoints=300,k=5,alpha=0.3)\n",
    "    #Q = NeuroQ(ACEnv.nactions, ACEnv.input_ranges, 30+nk, ACEnv.reward_ranges,ACEnv.deep_in,ACEnv.deep_out,alpha=0.3)\n",
    "    #Q = RNeuroQ(MCEnv.nactions, MCEnv.input_ranges, 10, MCEnv.reward_ranges,alpha=0.3)\n",
    "    #Q = SOMQ(nactions=MCEnv.nactions,size_x=20,size_y=20,input_ranges=MCEnv.input_ranges,alpha=0.3)\n",
    "    #Q = lwprQ(nactions=ACEnv.nactions,input_ranges=ACEnv.input_ranges)\n",
    "    # Get the Action Selector\n",
    "    As = e_greedy_selection(epsilon=0.000)\n",
    "    #As = e_softmax_selection(epsilon=0.1)\n",
    "\n",
    "    #Build the Agent\n",
    "    AC = FARLBase(Q,ACEnv,As,gamma=1.0)\n",
    "\n",
    "    AC.Environment.graphs=True#False\n",
    "    #AC.Environment.PlotPopulation(MC.Q)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(Episodes):\n",
    "        t1= time.clock()\n",
    "        result = AC.SARSAEpisode(1000)\n",
    "        #result = AC.QLearningEpisode(1000)\n",
    "        t2 = time.clock()-t1\n",
    "        #AC.SelectAction.epsilon = AC.SelectAction.epsilon * 0.9\n",
    "        AC.PlotLearningCurve(i,result[1],AC.SelectAction.epsilon)\n",
    "        #AC.Environment.PlotPopulation(MC.Q)\n",
    "        print('Episode',str(i),' Steps:',str(result[1]),'time',t2)\n",
    "        y.append(result[1])\n",
    "\n",
    "    return [x,y,nk]\n",
    "\n",
    "\n",
    "\n",
    "def Experiments():\n",
    "    results=[]\n",
    "    for i in range(0,10):\n",
    "        x = AcrobotExperiment(Episodes=1000,nk=i)\n",
    "        results.append( x )\n",
    "\n",
    "    cPickle.dump(results,open('acrobotresult1.dat','w'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    AcrobotExperiment(Episodes=1001,nk=0)\n",
    "    #Experiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VPython",
   "language": "python",
   "name": "vpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
